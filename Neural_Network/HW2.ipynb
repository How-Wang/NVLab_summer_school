{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unpackaged .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_train_image = \"Fashion_MNIST_data/train-images-idx3-ubyte.gz\"\n",
    "url_train_labels = \"Fashion_MNIST_data/train-labels-idx1-ubyte.gz\"\n",
    "url_test_image = \"Fashion_MNIST_data/t10k-images-idx3-ubyte.gz\"\n",
    "url_test_labels = \"Fashion_MNIST_data/t10k-labels-idx1-ubyte.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path_list = [url_train_image, url_train_labels, url_test_image, url_test_labels]\n",
    "    with gzip.open(path_list[0], 'rb') as f:\n",
    "        x_train = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28*28)/255.0\n",
    "    with gzip.open(path_list[1], 'rb') as f:\n",
    "        y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with gzip.open(path_list[2], 'rb') as f:\n",
    "        x_test = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28*28)/255.0\n",
    "    with gzip.open(path_list[3], 'rb') as f:\n",
    "        y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases for the first hidden layer\n",
    "        # self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        # use normal distribution to initialize weights\n",
    "        self.W1 = np.random.normal(0.0, pow(self.input_size, -0.5), (self.input_size, self.hidden_size))\n",
    "        self.b1 = np.zeros(self.hidden_size)\n",
    "        \n",
    "        # Initialize weights and biases for the output layer\n",
    "        # self.W3 = np.random.randn(self.hidden_size2, self.output_size)\n",
    "        self.W2 = np.random.normal(0.0, pow(self.hidden_size, -0.5), (self.hidden_size, self.output_size))\n",
    "        self.b2 = np.zeros(self.output_size)\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.train_score = []\n",
    "        self.test_loss = []\n",
    "        self.test_score = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward pass through the network\n",
    "        \n",
    "        ## Note: np.dot(X, self.W1) size is (N, hidden_size1),\n",
    "        ## self.b1 size is (hidden_size1)\n",
    "        ## so self.b1 will be broadcasted to the same shape as np.dot(X, self.W1)\n",
    "        ## self.z1 size is (N, hidden_size1)\n",
    "        \n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1) \n",
    "        # self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def relu(self, x):\n",
    "        ## Note: 0 will be broadcasted to the same shape as x is (N, hidden_size) \n",
    "        ## np.maximun is element-wise to compare which one is bigger \n",
    "        ## reture size is the same as x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        ## Note1: \n",
    "        ## x size is (N, output_size)\n",
    "        ## we need to sum over the second dimension, so we set axis=1, size is (N, 1)\n",
    "        ## and np.exp(x) size is (N, output_size)\n",
    "        ## so np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True) size is (N, output_size)\n",
    "        ## division here is broadcasted over the first dimension too\n",
    "        \n",
    "        # exp_x = np.exp(x)\n",
    "        # return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "        ## Note2: \n",
    "        ## subtracting the maximum value along the axis\n",
    "        ## is to prevent overflow when exponentiating large values\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, y_ture, y_pred):\n",
    "        ## Note 1:\n",
    "        ## dividing by y_true.shape[0] (the number of samples in the batch) \n",
    "        ## is a normalization step to ensure that the loss is independent of the batch size.\n",
    "        ## The loss function measures the average loss per sample in the batch.\n",
    "        ## Note 2: \n",
    "        ## original y size is (N, 1), y_pred size is (N, output_size), \n",
    "        ## we need to change y to one-hot encoding.\n",
    "        ## Note 3:\n",
    "        ## np.sum(y_ture * np.log(y_pred)) at the beginning is nan\n",
    "        ## because y_pred is nearly 0 at the beginning, and log(0) is not defined, so it is nan\n",
    "        ## we need to add a small number to y_pred to avoid this problem\n",
    "        epsilon = 1e-8\n",
    "        return -np.sum(y_ture * np.log(y_pred + epsilon)) / y_ture.shape[0]\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def backward(self, X, y_ture, lr=0.01):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        ## Note: dL_dz2 is calculated by the derivative of cross entropy and softmax\n",
    "        ## Note self.a2 size is (N, output_size), y_ture size is (N, output_size), dL_dz2 size is (N, output_size)\n",
    "        dL_dz2 = (self.a2 - y_ture) / m\n",
    "        ## Note: dL_dW2 is multiplied by backward and forward propagation\n",
    "        ## self.a2.T size is (hidden_size2, N), dL_dz3 size is (N, output_size), dL_dW3 size is (hidden_size2, output_size)\n",
    "        dL_dW2 = np.dot(self.a1.T, dL_dz2)\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dL_dz1 = np.dot(dL_dz2, self.W2.T) * self.relu_derivative(self.z1) / m\n",
    "        # dL_dz1 = np.dot(dL_dz2, self.W2.T) * self.sigmoid_derivative(self.z1) / m\n",
    "        dL_dW1 = np.dot(X.T, dL_dz1)\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W2 -= lr * dL_dW2\n",
    "        self.b2 -= lr * dL_db2\n",
    "        self.W1 -= lr * dL_dW1\n",
    "        self.b1 -= lr * dL_db1\n",
    "    \n",
    "    def precison(self, y_ture, y_pred):\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        y_ture_labels = np.argmax(y_ture, axis=1)\n",
    "        return np.sum(y_ture_labels == y_pred_labels) / len(y_ture)\n",
    "        \n",
    "    def train(self, X, y, x_test, y_test, epochs=100, lr=0.01, batch_size=32):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.cross_entropy_loss(y_batch, y_pred)\n",
    "                score = self.precison(y_batch, y_pred)\n",
    "                self.backward(X_batch, y_batch, lr)\n",
    "                \n",
    "            self.train_loss.append(loss)\n",
    "            self.train_score.append(score)\n",
    "            predict_result = self.predict(x_test, y_test)\n",
    "            \n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch+1}, \\nTrain  loss {loss:.4f}, precision {score:.4f}, \\nTest precision {predict_result:.4f}'\n",
    "                    )\n",
    "        \n",
    "        self.save_weights(f'Model/batch{batch_size}_epoch{epochs}_train{self.train_score[-1]}_test{self.test_score[-1]}_weights.npy')\n",
    "            \n",
    "        return self.train_loss, self.train_score, self.test_loss, self.test_score\n",
    "                \n",
    "                \n",
    "    def predict(self, X, y):\n",
    "        y_pred = self.forward(X)\n",
    "        loss = self.cross_entropy_loss(y, y_pred)\n",
    "        score = self.precison(y, y_pred)\n",
    "        self.test_loss.append(loss)\n",
    "        self.test_score.append(score)\n",
    "        # print(f'Test Loss: {loss:.4f}, precision: {score:.4f}')\n",
    "        return score\n",
    "        \n",
    "    def save_weights(self, file_path):\n",
    "        # Save model weights to a .npy file\n",
    "        weights = {\n",
    "            \"W1\": self.W1,\n",
    "            \"b1\": self.b1,\n",
    "            \"W2\": self.W2,\n",
    "            \"b2\": self.b2,\n",
    "        }\n",
    "        np.save(file_path, weights)\n",
    "\n",
    "    def load_weights(self, file_path):\n",
    "        # Load model weights from a .npy file\n",
    "        weights = np.load(file_path, allow_pickle=True).item()\n",
    "        self.W1 = weights[\"W1\"]\n",
    "        self.b1 = weights[\"b1\"]\n",
    "        self.W2 = weights[\"W2\"]\n",
    "        self.b2 = weights[\"b2\"]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 128\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "y_train_onehot = np.eye(OUTPUT_SIZE)[y_train]\n",
    "y_test_onehot = np.eye(OUTPUT_SIZE)[y_test]\n",
    "\n",
    "nn = FFNN(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "train_loss, train_score, test_loss, test_score = nn.train(X_train, y_train_onehot, x_test, y_test_onehot, EPOCHS, LEARNING_RATE, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "fig.add_subplot(2,2,(1,2))\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(range(1, EPOCHS+1), train_loss, label=\"train_loss\")\n",
    "plt.plot(range(1, EPOCHS+1), test_loss, label=\"test_loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "fig.add_subplot(2,2,(3,4))\n",
    "plt.plot(range(1, EPOCHS+1), train_score, label=\"train_acc\")\n",
    "plt.plot(range(1, EPOCHS+1), test_score, label=\"test_acc\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(f'result_picture/batch{BATCH_SIZE}_epoch{EPOCHS}_train{train_score[-1]}_test{test_score[-1]}.png')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run prdiction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.load_weights('Model/batch100_epoch300_train0.91_test0.8842_weights.npy')\n",
    "nn.predict(x_test, y_test_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
